"""
NovaMindè®­ç»ƒæ¡†æ¶ä½¿ç”¨ç¤ºä¾‹

å±•ç¤ºå¦‚ä½•ä½¿ç”¨NovaMindè®­ç»ƒæ¡†æ¶è¿›è¡Œä¸åŒç±»å‹çš„æ¨¡å‹è®­ç»ƒï¼š
1. åŸºç¡€è¯­è¨€æ¨¡å‹è®­ç»ƒ
2. æŒ‡ä»¤å¾®è°ƒè®­ç»ƒ
3. å¯¹è¯æ¨¡å‹è®­ç»ƒ
4. LoRAå¾®è°ƒ
5. å®æ—¶ç›‘æ§
"""

import asyncio
import json
from pathlib import Path
from typing import Dict, Any

from .trainer import TrainingManager, TrainingStatus
from .llm_trainer import LLMTrainingConfig, LLMTrainer
from .monitor import training_monitor, MetricsCallback


def create_sample_data():
    """åˆ›å»ºç¤ºä¾‹æ•°æ®"""
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    data_dir = Path("./sample_data")
    data_dir.mkdir(exist_ok=True)
    
    # 1. çº¯æ–‡æœ¬æ•°æ®
    text_data = [
        "äººå·¥æ™ºèƒ½æ˜¯è®¡ç®—æœºç§‘å­¦çš„ä¸€ä¸ªåˆ†æ”¯ï¼Œè‡´åŠ›äºåˆ›å»ºèƒ½å¤Ÿæ‰§è¡Œé€šå¸¸éœ€è¦äººç±»æ™ºèƒ½çš„ä»»åŠ¡çš„ç³»ç»Ÿã€‚",
        "æœºå™¨å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½çš„ä¸€ä¸ªå­é›†ï¼Œå®ƒä½¿è®¡ç®—æœºèƒ½å¤Ÿåœ¨æ²¡æœ‰æ˜ç¡®ç¼–ç¨‹çš„æƒ…å†µä¸‹å­¦ä¹ å’Œæ”¹è¿›ã€‚",
        "æ·±åº¦å­¦ä¹ æ˜¯æœºå™¨å­¦ä¹ çš„ä¸€ä¸ªåˆ†æ”¯ï¼Œä½¿ç”¨ç¥ç»ç½‘ç»œæ¥æ¨¡æ‹Ÿäººè„‘çš„å­¦ä¹ è¿‡ç¨‹ã€‚",
        "è‡ªç„¶è¯­è¨€å¤„ç†æ˜¯äººå·¥æ™ºèƒ½çš„ä¸€ä¸ªé¢†åŸŸï¼Œä¸“æ³¨äºè®¡ç®—æœºç†è§£å’Œç”Ÿæˆäººç±»è¯­è¨€çš„èƒ½åŠ›ã€‚",
        "è®¡ç®—æœºè§†è§‰æ˜¯äººå·¥æ™ºèƒ½çš„ä¸€ä¸ªåˆ†æ”¯ï¼Œä½¿è®¡ç®—æœºèƒ½å¤Ÿä»å›¾åƒå’Œè§†é¢‘ä¸­ç†è§£å’Œæå–ä¿¡æ¯ã€‚"
    ]
    
    with open(data_dir / "text_data.json", "w", encoding="utf-8") as f:
        json.dump([{"text": text} for text in text_data], f, ensure_ascii=False, indent=2)
    
    # 2. æŒ‡ä»¤æ•°æ®
    instruction_data = [
        {
            "instruction": "è§£é‡Šä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½",
            "response": "äººå·¥æ™ºèƒ½ï¼ˆAIï¼‰æ˜¯è®¡ç®—æœºç§‘å­¦çš„ä¸€ä¸ªåˆ†æ”¯ï¼Œè‡´åŠ›äºåˆ›å»ºèƒ½å¤Ÿæ‰§è¡Œé€šå¸¸éœ€è¦äººç±»æ™ºèƒ½çš„ä»»åŠ¡çš„ç³»ç»Ÿã€‚è¿™äº›ä»»åŠ¡åŒ…æ‹¬å­¦ä¹ ã€æ¨ç†ã€é—®é¢˜è§£å†³ã€æ„ŸçŸ¥å’Œè¯­è¨€ç†è§£ã€‚"
        },
        {
            "instruction": "ä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ï¼Ÿ",
            "response": "æœºå™¨å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½çš„ä¸€ä¸ªå­é›†ï¼Œå®ƒä½¿è®¡ç®—æœºèƒ½å¤Ÿåœ¨æ²¡æœ‰æ˜ç¡®ç¼–ç¨‹çš„æƒ…å†µä¸‹å­¦ä¹ å’Œæ”¹è¿›ã€‚é€šè¿‡åˆ†ææ•°æ®ï¼Œæœºå™¨å­¦ä¹ ç®—æ³•å¯ä»¥è¯†åˆ«æ¨¡å¼å¹¶åšå‡ºé¢„æµ‹æˆ–å†³ç­–ã€‚"
        },
        {
            "instruction": "æ·±åº¦å­¦ä¹ ä¸ä¼ ç»Ÿæœºå™¨å­¦ä¹ æœ‰ä»€ä¹ˆåŒºåˆ«ï¼Ÿ",
            "response": "æ·±åº¦å­¦ä¹ æ˜¯æœºå™¨å­¦ä¹ çš„ä¸€ä¸ªåˆ†æ”¯ï¼Œä½¿ç”¨å¤šå±‚ç¥ç»ç½‘ç»œæ¥æ¨¡æ‹Ÿäººè„‘çš„å­¦ä¹ è¿‡ç¨‹ã€‚ä¸ä¼ ç»Ÿæœºå™¨å­¦ä¹ ç›¸æ¯”ï¼Œæ·±åº¦å­¦ä¹ èƒ½å¤Ÿè‡ªåŠ¨å­¦ä¹ ç‰¹å¾ï¼Œè€Œä¼ ç»Ÿæ–¹æ³•é€šå¸¸éœ€è¦æ‰‹åŠ¨ç‰¹å¾å·¥ç¨‹ã€‚"
        }
    ]
    
    with open(data_dir / "instruction_data.json", "w", encoding="utf-8") as f:
        json.dump(instruction_data, f, ensure_ascii=False, indent=2)
    
    # 3. å¯¹è¯æ•°æ®
    conversation_data = [
        [
            {"role": "user", "content": "ä½ å¥½ï¼Œè¯·ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±"},
            {"role": "assistant", "content": "ä½ å¥½ï¼æˆ‘æ˜¯NovaMind AIåŠ©æ‰‹ï¼Œä¸€ä¸ªåŸºäºå…ˆè¿›äººå·¥æ™ºèƒ½æŠ€æœ¯å¼€å‘çš„æ™ºèƒ½å¯¹è¯ç³»ç»Ÿã€‚æˆ‘å¯ä»¥å¸®åŠ©æ‚¨å›ç­”é—®é¢˜ã€æä¾›ä¿¡æ¯ã€ååŠ©å†™ä½œç­‰å¤šç§ä»»åŠ¡ã€‚"}
        ],
        [
            {"role": "user", "content": "ä½ èƒ½åšä»€ä¹ˆï¼Ÿ"},
            {"role": "assistant", "content": "æˆ‘å¯ä»¥è¿›è¡Œè‡ªç„¶è¯­è¨€å¯¹è¯ã€å›ç­”é—®é¢˜ã€ååŠ©å†™ä½œã€ä»£ç ç”Ÿæˆã€æ•°æ®åˆ†æç­‰å¤šç§ä»»åŠ¡ã€‚æˆ‘ä¼šå°½åŠ›ä¸ºæ‚¨æä¾›å‡†ç¡®ã€æœ‰ç”¨çš„å¸®åŠ©ã€‚"}
        ],
        [
            {"role": "user", "content": "è¯·è§£é‡Šä¸€ä¸‹æœºå™¨å­¦ä¹ çš„åŸºæœ¬æ¦‚å¿µ"},
            {"role": "assistant", "content": "æœºå™¨å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½çš„ä¸€ä¸ªæ ¸å¿ƒåˆ†æ”¯ï¼Œå®ƒä½¿è®¡ç®—æœºèƒ½å¤Ÿä»æ•°æ®ä¸­å­¦ä¹ å¹¶æ”¹è¿›æ€§èƒ½ï¼Œè€Œæ— éœ€æ˜ç¡®ç¼–ç¨‹ã€‚ä¸»è¦åŒ…æ‹¬ç›‘ç£å­¦ä¹ ã€æ— ç›‘ç£å­¦ä¹ å’Œå¼ºåŒ–å­¦ä¹ ä¸‰ç§ç±»å‹ã€‚"}
        ]
    ]
    
    with open(data_dir / "conversation_data.json", "w", encoding="utf-8") as f:
        json.dump(conversation_data, f, ensure_ascii=False, indent=2)
    
    print("ç¤ºä¾‹æ•°æ®å·²åˆ›å»ºåœ¨ ./sample_data ç›®å½•ä¸­")
    return data_dir


async def example_basic_training():
    """ç¤ºä¾‹1ï¼šåŸºç¡€è¯­è¨€æ¨¡å‹è®­ç»ƒ"""
    print("\n=== ç¤ºä¾‹1ï¼šåŸºç¡€è¯­è¨€æ¨¡å‹è®­ç»ƒ ===")
    
    # åˆ›å»ºè®­ç»ƒé…ç½®
    config = LLMTrainingConfig(
        model_name="basic_lm_training",
        model_type="llm",
        model_path="microsoft/DialoGPT-small",  # ä½¿ç”¨å°å‹æ¨¡å‹ä½œä¸ºç¤ºä¾‹
        dataset_path="./sample_data/text_data.json",
        output_dir="./outputs/basic_training",
        batch_size=4,
        learning_rate=5e-5,
        num_epochs=3,
        max_length=128,
        log_interval=5,
        eval_interval=10,
        save_interval=20
    )
    
    # åˆ›å»ºè®­ç»ƒå™¨
    trainer = LLMTrainer(config)
    
    # æ·»åŠ ç›‘æ§å›è°ƒ
    trainer.add_callback(MetricsCallback("basic_training"))
    
    # å¯åŠ¨è®­ç»ƒ
    training_id = training_manager.start_training("basic_training", trainer)
    print(f"åŸºç¡€è®­ç»ƒå·²å¯åŠ¨ï¼ŒID: {training_id}")
    
    # ç­‰å¾…è®­ç»ƒå®Œæˆ
    while True:
        status = training_manager.get_training_status(training_id)
        if status and status['status'] in [TrainingStatus.COMPLETED, TrainingStatus.FAILED]:
            print(f"è®­ç»ƒå®Œæˆï¼ŒçŠ¶æ€: {status['status']}")
            break
        await asyncio.sleep(5)


async def example_instruction_tuning():
    """ç¤ºä¾‹2ï¼šæŒ‡ä»¤å¾®è°ƒè®­ç»ƒ"""
    print("\n=== ç¤ºä¾‹2ï¼šæŒ‡ä»¤å¾®è°ƒè®­ç»ƒ ===")
    
    # åˆ›å»ºè®­ç»ƒé…ç½®
    config = LLMTrainingConfig(
        model_name="instruction_tuning",
        model_type="llm",
        model_path="microsoft/DialoGPT-small",
        dataset_path="./sample_data/instruction_data.json",
        output_dir="./outputs/instruction_tuning",
        batch_size=2,
        learning_rate=3e-5,
        num_epochs=2,
        max_length=256,
        instruction_tuning=True,
        log_interval=3,
        eval_interval=5,
        save_interval=10
    )
    
    # åˆ›å»ºè®­ç»ƒå™¨
    trainer = LLMTrainer(config)
    
    # æ·»åŠ ç›‘æ§å›è°ƒ
    trainer.add_callback(MetricsCallback("instruction_tuning"))
    
    # å¯åŠ¨è®­ç»ƒ
    training_id = training_manager.start_training("instruction_tuning", trainer)
    print(f"æŒ‡ä»¤å¾®è°ƒè®­ç»ƒå·²å¯åŠ¨ï¼ŒID: {training_id}")
    
    # ç­‰å¾…è®­ç»ƒå®Œæˆ
    while True:
        status = training_manager.get_training_status(training_id)
        if status and status['status'] in [TrainingStatus.COMPLETED, TrainingStatus.FAILED]:
            print(f"è®­ç»ƒå®Œæˆï¼ŒçŠ¶æ€: {status['status']}")
            break
        await asyncio.sleep(5)


async def example_conversation_training():
    """ç¤ºä¾‹3ï¼šå¯¹è¯æ¨¡å‹è®­ç»ƒ"""
    print("\n=== ç¤ºä¾‹3ï¼šå¯¹è¯æ¨¡å‹è®­ç»ƒ ===")
    
    # åˆ›å»ºè®­ç»ƒé…ç½®
    config = LLMTrainingConfig(
        model_name="conversation_training",
        model_type="llm",
        model_path="microsoft/DialoGPT-small",
        dataset_path="./sample_data/conversation_data.json",
        output_dir="./outputs/conversation_training",
        batch_size=2,
        learning_rate=2e-5,
        num_epochs=2,
        max_length=512,
        chat_template="chatml",
        log_interval=3,
        eval_interval=5,
        save_interval=10
    )
    
    # åˆ›å»ºè®­ç»ƒå™¨
    trainer = LLMTrainer(config)
    
    # æ·»åŠ ç›‘æ§å›è°ƒ
    trainer.add_callback(MetricsCallback("conversation_training"))
    
    # å¯åŠ¨è®­ç»ƒ
    training_id = training_manager.start_training("conversation_training", trainer)
    print(f"å¯¹è¯è®­ç»ƒå·²å¯åŠ¨ï¼ŒID: {training_id}")
    
    # ç­‰å¾…è®­ç»ƒå®Œæˆ
    while True:
        status = training_manager.get_training_status(training_id)
        if status and status['status'] in [TrainingStatus.COMPLETED, TrainingStatus.FAILED]:
            print(f"è®­ç»ƒå®Œæˆï¼ŒçŠ¶æ€: {status['status']}")
            break
        await asyncio.sleep(5)


async def example_lora_training():
    """ç¤ºä¾‹4ï¼šLoRAå¾®è°ƒè®­ç»ƒ"""
    print("\n=== ç¤ºä¾‹4ï¼šLoRAå¾®è°ƒè®­ç»ƒ ===")
    
    # åˆ›å»ºLoRAé…ç½®
    lora_config = LoRAConfig(
        r=8,
        alpha=16,
        dropout=0.1,
        target_modules=["q_proj", "v_proj"],
        bias="none",
        task_type="CAUSAL_LM"
    )
    
    # åˆ›å»ºè®­ç»ƒé…ç½®
    config = LLMTrainingConfig(
        model_name="lora_training",
        model_type="llm",
        model_path="microsoft/DialoGPT-small",
        dataset_path="./sample_data/instruction_data.json",
        output_dir="./outputs/lora_training",
        batch_size=4,
        learning_rate=1e-4,
        num_epochs=2,
        max_length=256,
        instruction_tuning=True,
        use_lora=True,
        lora_config=lora_config,
        log_interval=3,
        eval_interval=5,
        save_interval=10
    )
    
    # åˆ›å»ºè®­ç»ƒå™¨
    trainer = LLMTrainer(config)
    
    # æ·»åŠ ç›‘æ§å›è°ƒ
    trainer.add_callback(MetricsCallback("lora_training"))
    
    # å¯åŠ¨è®­ç»ƒ
    training_id = training_manager.start_training("lora_training", trainer)
    print(f"LoRAè®­ç»ƒå·²å¯åŠ¨ï¼ŒID: {training_id}")
    
    # ç­‰å¾…è®­ç»ƒå®Œæˆ
    while True:
        status = training_manager.get_training_status(training_id)
        if status and status['status'] in [TrainingStatus.COMPLETED, TrainingStatus.FAILED]:
            print(f"è®­ç»ƒå®Œæˆï¼ŒçŠ¶æ€: {status['status']}")
            break
        await asyncio.sleep(5)


def example_text_generation():
    """ç¤ºä¾‹5ï¼šæ–‡æœ¬ç”Ÿæˆæµ‹è¯•"""
    print("\n=== ç¤ºä¾‹5ï¼šæ–‡æœ¬ç”Ÿæˆæµ‹è¯• ===")
    
    # åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹ï¼ˆè¿™é‡Œä½¿ç”¨ç¤ºä¾‹ï¼‰
    config = LLMTrainingConfig(
        model_name="test_generation",
        model_type="llm",
        model_path="microsoft/DialoGPT-small",
        dataset_path="./sample_data/text_data.json",
        output_dir="./outputs/test_generation"
    )
    
    trainer = LLMTrainer(config)
    
    # æµ‹è¯•æç¤º
    test_prompts = [
        "äººå·¥æ™ºèƒ½çš„æœªæ¥å‘å±•",
        "æœºå™¨å­¦ä¹ çš„åº”ç”¨é¢†åŸŸ",
        "æ·±åº¦å­¦ä¹ çš„æŠ€æœ¯ä¼˜åŠ¿"
    ]
    
    print("ç”Ÿæˆæ–‡æœ¬ç¤ºä¾‹ï¼š")
    for prompt in test_prompts:
        generated = trainer.generate_text(prompt, max_length=100, temperature=0.7)
        print(f"æç¤º: {prompt}")
        print(f"ç”Ÿæˆ: {generated}")
        print("-" * 50)


def example_monitoring():
    """ç¤ºä¾‹6ï¼šç›‘æ§ç³»ç»Ÿæ¼”ç¤º"""
    print("\n=== ç¤ºä¾‹6ï¼šç›‘æ§ç³»ç»Ÿæ¼”ç¤º ===")
    
    # å¯åŠ¨ç›‘æ§æœåŠ¡å™¨
    training_monitor.start_background()
    print("ç›‘æ§æœåŠ¡å™¨å·²å¯åŠ¨ï¼Œè®¿é—® http://localhost:8080 æŸ¥çœ‹ä»ªè¡¨æ¿")
    
    # æ˜¾ç¤ºè®­ç»ƒåˆ—è¡¨
    trainings = training_manager.list_trainings()
    print(f"å½“å‰è®­ç»ƒä»»åŠ¡æ•°é‡: {len(trainings)}")
    
    for training in trainings:
        print(f"è®­ç»ƒID: {training['id']}")
        print(f"çŠ¶æ€: {training['status']['status']}")
        print(f"æ´»è·ƒ: {training['active']}")
        print("-" * 30)


async def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ NovaMindè®­ç»ƒæ¡†æ¶ç¤ºä¾‹")
    print("=" * 50)
    
    # åˆ›å»ºç¤ºä¾‹æ•°æ®
    create_sample_data()
    
    # å¯åŠ¨ç›‘æ§ç³»ç»Ÿ
    training_monitor.start_background()
    print("ç›‘æ§ç³»ç»Ÿå·²å¯åŠ¨: http://localhost:8080")
    
    try:
        # è¿è¡Œå„ç§è®­ç»ƒç¤ºä¾‹
        await example_basic_training()
        await example_instruction_tuning()
        await example_conversation_training()
        await example_lora_training()
        
        # æ–‡æœ¬ç”Ÿæˆæµ‹è¯•
        example_text_generation()
        
        # ç›‘æ§æ¼”ç¤º
        example_monitoring()
        
        print("\nâœ… æ‰€æœ‰ç¤ºä¾‹è¿è¡Œå®Œæˆï¼")
        print("è®¿é—® http://localhost:8080 æŸ¥çœ‹è®­ç»ƒç›‘æ§ä»ªè¡¨æ¿")
        
    except Exception as e:
        print(f"âŒ è¿è¡Œç¤ºä¾‹æ—¶å‡ºé”™: {e}")
    
    # ä¿æŒç›‘æ§æœåŠ¡å™¨è¿è¡Œ
    try:
        while True:
            await asyncio.sleep(1)
    except KeyboardInterrupt:
        print("\nğŸ‘‹ ç¨‹åºå·²é€€å‡º")


if __name__ == "__main__":
    asyncio.run(main()) 